as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# then load the pdf or URLs we want in the publist # if you want to update new pub you NEED to add pdf link here or it will not show up
pdf_urls_file<-read.csv("publications/pdf_urls.csv", sep = ",", header = T)
pdf_urls_file
html_1
# pull from google
html_1 <- get_publications(MaxLarter)
# pull pdf_urls matching GS titles
# first write this to file
write.csv(html_1 %>%
as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# then load the pdf or URLs we want in the publist # if you want to update new pub you NEED to add pdf link here or it will not show up
pdf_urls_file<-read.csv("publications/pdf_urls.csv", sep = ",", header = T)
html_1_2<-merge(html_1, pdf_urls_file, by = "title")
html_1_2
# convert to htlm table - the ugly way ;)
html_1_2 <- html_1_2 %>% mutate(author= ifelse(!str_detect(author, 'Larter'),
str_replace_all(author, ", \\.\\.\\.", ", M Larter, \\.\\.\\."),author)
) # this will add my name before the "et al." in long author lists when I'm towards the end
html_2 <- html_1_2 %>%
as_tibble %>% arrange(desc(year)) %>%
mutate(
#    author=str_replace_all(author, " (\\S) ", "\\1 "),
author=str_replace_all(author, "([A-Z]) ([A-Z]) ", "\\1\\2 "),
author=str_replace_all(author, ", \\.\\.\\.", " et al."),
author=str_replace_all(author, "M Larter", "<b>M Larter</b>"), # make my name fat
author=str_replace_all(author, "MD Larter", "<b>MD Larter</b>") # make my name fat
) %>% split(.$year) %>%
map(function(x){
x <- x %>%
glue_data('<tr><td width="100%">{author} ({year}) <a href="https://scholar.google.com/scholar?oi=bibs&cluster={cid}&btnI=1&hl=en" target="_blank">{title}</a>, {journal}, {number} [<i>{cites} citation(s)</i>] <a href="{pdf_url}" target="_blank">(pdf)</a> </td></tr>') %>%
str_replace_all("(, )+</p>", "</p>") %>%
char2html()
x <- c('<table><tbody>', x, '</tbody></table>')
return(x);
}) %>% rev
html_3 <- map2(names(html_2) %>% paste0("<h3>", ., "</h3>"), html_2, c) %>% unlist
html_4 <- c(
paste0('<p><i>Last updated ',
format(Sys.Date(), format="%d/%m/%Y"),
' &ndash; Pulled automatically from my <a href="https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en">Google Scholar profile</a>. See <a href="https://thackl.github.io/automatically-update-publications-with-R-scholar" target="_blank">this post</a> for how it works. See the code adapted to my usage <a href = "https://github.com/MaxLarter/maxlarter.github.io/blob/master/.R.Rmd" target="_blank">here</a>.</i></p>'), html_3)
knitr::opts_chunk$set(echo = TRUE)
# Knit the HTML version
rmarkdown::render("data/cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv_ML_new.html")
knitr::opts_chunk$set(
results='asis',
echo = FALSE
)
CRANpkg <- function (pkg) {
cran <- "https://CRAN.R-project.org/package"
fmt <- "[%s](%s=%s)"
sprintf(fmt, pkg, cran, pkg)
}
Biocpkg <- function (pkg) {
sprintf("[%s](http://bioconductor.org/packages/%s)", pkg, pkg)
}
library(glue)
library(tidyverse)
# Set this to true to have links turned into footnotes at the end of the document
PDF_EXPORT <- FALSE
# Holds all the links that were inserted for placement at the end
links <- c()
find_link <- regex("
\\[   # Grab opening square bracket
.+?   # Find smallest internal text as possible
\\]   # Closing square bracket
\\(   # Opening parenthesis
.+?   # Link text, again as small as possible
\\)   # Closing parenthesis
",
comments = TRUE)
sanitize_links <- function(text){
if(PDF_EXPORT){
str_extract_all(text, find_link) %>%
pluck(1) %>%
walk(function(link_from_text){
title <- link_from_text %>% str_extract('\\[.+\\]') %>% str_remove_all('\\[|\\]')
link <- link_from_text %>% str_extract('\\(.+\\)') %>% str_remove_all('\\(|\\)')
# add link to links array
links <<- c(links, link)
# Build replacement text
new_text <- glue('{title}<sup>{length(links)}</sup>')
# Replace text
text <<- text %>% str_replace(fixed(link_from_text), new_text)
})
}
text
}
# Takes a single row of dataframe corresponding to a position
# turns it into markdown, and prints the result to console.
build_position_from_df <- function(pos_df){
missing_start <- pos_df$start == 'N/A'
dates_same <- pos_df$end == pos_df$start
if (pos_df$end == 9999) {
pos_df$end = "present"
}
if(any(c(missing_start,dates_same))){
timeline <- pos_df$end
} else {
timeline <- glue('{pos_df$end} - {pos_df$start}')
}
descriptions <- pos_df[str_detect(names(pos_df), 'description')] %>%
as.list() %>%
map_chr(sanitize_links)
# Make sure we only keep filled in descriptions
description_bullets <- paste('-', descriptions[descriptions != 'N/A'], collapse = '\n')
if (length(description_bullets) == 1 && description_bullets == "- ") {
description_bullets <- ""
}
glue(
"### {sanitize_links(pos_df$title)}
{pos_df$loc}
{pos_df$institution}
{timeline}
{description_bullets}
"
) %>% print()
}
# Takes nested position data and a given section id
# and prints all the positions in that section to console
print_section <- function(position_data, section_id){
x <- position_data %>%
filter(section == section_id) %>%
pull(data)
prese <- " - "
xx <- list()
for (i in seq_along(x)) {
y = x[[i]]
y <- cbind(y, start2 = as.character(y$start))
y <- cbind(y, end2 = as.character(y$end))
se <- paste(y$start, "-", y$end, collapse = " ")
if (prese == se) {
y$start2 = ""
y$end2 = ""
} else {
prese = se
}
xx[[i]] <- select(y, -c(start, end)) %>%
rename(start=start2, end=end2)
}
xx %>%
purrr::walk(build_position_from_df)
}
fill_nas <- function(column){
ifelse(is.na(column), 'N/A', column)
}
source("citation.R", local = knitr::knit_global())
# or sys.source("your-script.R", envir = knitr::knit_global())
# Load csv with position info
position_data <- read_csv('CV_larter.csv') %>%
mutate_all(fill_nas) %>%
arrange(order, desc(end)) %>%
mutate(id = 1:n()) %>%
nest(data = c(-id, -section))
# When in export mode the little dots are unaligned, so fix that.
if(PDF_EXPORT){
cat("
<style>
:root{
--decorator-outer-offset-left: -6.5px;
}
</style>")
}
# When in export mode the little dots are unaligned, so fix that.
if(PDF_EXPORT){
cat("View this CV online with links at _maxlarter.github.io/cv_")
}
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv_ML_new.html")
# Knit the PDF version to temporary html location
tmp_html_cv_loc <- fs::file_temp(ext = ".html")
rmarkdown::render("data/cv/cv.rmd",
params = list(pdf_mode = TRUE),
output_file = tmp_html_cv_loc)
# Convert to PDF using Pagedown
pagedown::chrome_print(input = "cv/cv_ML_new.html",
output = "cv.pdf")
# Convert to PDF using Pagedown
pagedown::chrome_print(input = "cv_ML_new.html",
output = "cv.pdf")
# Convert to PDF using Pagedown
pagedown::chrome_print("cv_ML_new.html", "CV_Maximilian Larter  EN - 2021_09_09.pdf")
# Convert to PDF using Pagedown
pagedown::chrome_print("cv/cv_ML_new.html", "CV_Maximilian Larter  EN - 2021_09_09.pdf")
Sys.Date()
# Convert to PDF using Pagedown
pdfCVname<-paste0("CV_Maximilian Larter  EN -",Sys.Date(),".pdf")
pdfCVname
# Knit the HTML version
rmarkdown::render("cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv/cv.html")
# Knit the HTML version
rmarkdown::render("cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
rmarkdown::render("cv/cv - priv.Rmd",
params = list(pdf_mode = TRUE),
output_file = "cv_priv.html")
# Convert to PDF using Pagedown
pdfCVname<-paste0("CV_Maximilian Larter  EN - ",Sys.Date(),".pdf")
pagedown::chrome_print("cv/cv_priv.html", pdfCVname)
knitr::opts_chunk$set(echo = TRUE)
# the first time you may need to re-reun this:
#install.packages("scholar")
#install.packages("tidyverse")
#install.packages("glue")
library(scholar)
library(tidyverse)
library(glue)
library(vitae)
#devtools::install_github("nstrayer/datadrivencv")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv2.html")
install.packages("ggimage")
library(ggimage)
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv2.html")
rmarkdown::render("cv/cv - priv.Rmd",
params = list(pdf_mode = TRUE),
output_file = "cv/cv_priv.html")
library(knitr)
opts_chunk$set(echo = TRUE)
# the first time you may need to re-reun this:
#install.packages("scholar")
#install.packages("tidyverse")
#install.packages("glue")
library(scholar)
library(tidyverse)
library(glue)
# escape some special chars, german umlauts, ...
char2html <- function(x){
dictionary <- data.frame(
symbol = c("ä","ö","ü","Ä", "Ö", "Ü", "ß", "á", "ñ", "é", "è", "«", "»", "à"),
html = c("&auml;","&ouml;", "&uuml;","&Auml;",
"&Ouml;", "&Uuml;","&szlig;", "&aacute;", "&ntilde;", "&eacute;", "&egrave;", "&laquo;", "&raquo;", "&agrave;"))
for(i in 1:dim(dictionary)[1]){
x <- gsub(dictionary$symbol[i],dictionary$html[i],x)
}
x
}
# my google scholar user id from my profile url
# https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en
MaxLarter <- "-WJKy_EAAAAJ&hl"
# pull from google
html_1 <- get_publications(MaxLarter)
# pull pdf_urls matching GS titles
# first write this to file
write.csv(html_1 %>%
as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# pull pdf_urls matching GS titles
# first write this to file
write.csv(html_1 %>%
as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# my google scholar user id from my profile url
# https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en
MaxLarter <- "-WJKy_EAAAAJ&hl"
# pull from google
html_1 <- get_publications(MaxLarter)
# pull pdf_urls matching GS titles
# first write this to file
write.csv(html_1 %>%
as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# then load the pdf or URLs we want in the pub list # if you want to update new pub you NEED to add pdf link here or it will not show up
pdf_urls_file<-read.csv("publications/pdf_urls.csv", sep = ",", header = T)
html_1_2<-merge(html_1, pdf_urls_file, by = "title")
# convert to htlm table - the ugly way ;)
html_1_2 <- html_1_2 %>% mutate(author= ifelse(!str_detect(author, 'Larter'),
str_replace_all(author, ", \\.\\.\\.", ", M Larter, \\.\\.\\."),author)
) # this will add my name before the "et al." in long author lists when I'm towards the end
html_2 <- html_1_2 %>%
as_tibble %>% arrange(desc(year)) %>%
mutate(
#    author=str_replace_all(author, " (\\S) ", "\\1 "),
author=str_replace_all(author, "([A-Z]) ([A-Z]) ", "\\1\\2 "),
author=str_replace_all(author, ", \\.\\.\\.", " et al."),
author=str_replace_all(author, "M Larter", "<b>M Larter</b>"), # make my name fat
author=str_replace_all(author, "MD Larter", "<b>MD Larter</b>") # make my name fat
) %>% split(.$year) %>%
map(function(x){
x <- x %>%
glue_data('<tr><td width="100%">{author} ({year}) <a href="https://scholar.google.com/scholar?oi=bibs&cluster={cid}&btnI=1&hl=en" target="_blank">{title}</a>, {journal}, {number} [<i>{cites} citation(s)</i>] <a href="{pdf_url}" target="_blank">(pdf)</a> </td></tr>') %>%
str_replace_all("(, )+</p>", "</p>") %>%
char2html()
x <- c('<table><tbody>', x, '</tbody></table>')
return(x);
}) %>% rev
html_3 <- map2(names(html_2) %>% paste0("<h3>", ., "</h3>"), html_2, c) %>% unlist
html_4 <- c(
paste0('<p><i>Last updated ',
format(Sys.Date(), format="%d/%m/%Y"),
' &ndash; Pulled automatically from my <a href="https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en">Google Scholar profile</a>. See <a href="https://thackl.github.io/automatically-update-publications-with-R-scholar" target="_blank">this post</a> for how it works. See the code adapted to my usage <a href = "https://github.com/MaxLarter/maxlarter.github.io/blob/master/.R.Rmd" target="_blank">here</a>.</i></p>'), html_3)
html_4
html_3
c
html_2
html_1
?get_publications
knitr::opts_chunk$set(echo = TRUE)
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
co_auth<-get_coauthors(MaxLarter, n_coauthors = 5)
plot_coauthors(co_auth)
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
knitr::opts_chunk$set(echo = TRUE)
# the first time you may need to re-reun this:
#install.packages("scholar")
#install.packages("tidyverse")
#install.packages("glue")
# install.packages("ggimage")
library(scholar)
library(tidyverse)
library(glue)
library(vitae)
library(ggimage)
#devtools::install_github("nstrayer/datadrivencv")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the HTML version
rmarkdown::render("cv/cv.Rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
rmarkdown::render("cv/cv - priv.Rmd",
params = list(pdf_mode = TRUE),
output_file = "cv/cv_priv.html")
# Convert to PDF using Pagedown
pdfCVname<-paste0("cv/CV_Maximilian Larter  EN - ",Sys.Date(),".pdf")
pagedown::chrome_print("cv/cv_priv.html", pdfCVname)
# Convert to PDF using Pagedown
pdfCVname<-paste0("cv/CV_Maximilian Larter  EN - ",Sys.Date(),".pdf")
pagedown::chrome_print("cv/cv_priv.html", pdfCVname)
# Convert to PDF using Pagedown
pdfCVname<-paste0("cv/CV_Maximilian Larter  EN - ",Sys.Date(),".pdf")
pagedown::chrome_print("cv/cv_priv.html", pdfCVname)
dat_inserted2
library(knitr)
opts_chunk$set(echo = TRUE)
# the first time you may need to re-reun this:
#install.packages("scholar")
#install.packages("tidyverse")
#install.packages("glue")
library(scholar)
library(tidyverse)
library(glue)
# escape some special chars, german umlauts, ...
char2html <- function(x){
dictionary <- data.frame(
symbol = c("ä","ö","ü","Ä", "Ö", "Ü", "ß", "á", "ñ", "é", "è", "«", "»", "à"),
html = c("&auml;","&ouml;", "&uuml;","&Auml;",
"&Ouml;", "&Uuml;","&szlig;", "&aacute;", "&ntilde;", "&eacute;", "&egrave;", "&laquo;", "&raquo;", "&agrave;"))
for(i in 1:dim(dictionary)[1]){
x <- gsub(dictionary$symbol[i],dictionary$html[i],x)
}
x
}
# my google scholar user id from my profile url
# https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en
MaxLarter <- "-WJKy_EAAAAJ&hl"
# pull from google
html_1 <- get_publications(MaxLarter)
# pull pdf_urls matching GS titles
# first write this to file
write.csv(html_1 %>%
as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# then load the pdf or URLs we want in the pub list # if you want to update new pub you NEED to add pdf link here or it will not show up
pdf_urls_file<-read.csv("publications/pdf_urls.csv", sep = ",", header = T)
html_1_2<-merge(html_1, pdf_urls_file, by = "title")
# convert to htlm table - the ugly way ;)
html_1_2 <- html_1_2 %>% mutate(author= ifelse(!str_detect(author, 'Larter'),
str_replace_all(author, ", \\.\\.\\.", ", M Larter, \\.\\.\\."),author)
) # this will add my name before the "et al." in long author lists when I'm towards the end
html_2 <- html_1_2 %>%
as_tibble %>% arrange(desc(year)) %>%
mutate(
#    author=str_replace_all(author, " (\\S) ", "\\1 "),
author=str_replace_all(author, "([A-Z]) ([A-Z]) ", "\\1\\2 "),
author=str_replace_all(author, ", \\.\\.\\.", " et al."),
author=str_replace_all(author, "M Larter", "<b>M Larter</b>"), # make my name fat
author=str_replace_all(author, "MD Larter", "<b>MD Larter</b>") # make my name fat
) %>% split(.$year) %>%
map(function(x){
x <- x %>%
glue_data('<tr><td width="100%">{author} ({year}) <a href="https://scholar.google.com/scholar?oi=bibs&cluster={cid}&btnI=1&hl=en" target="_blank">{title}</a>, {journal}, {number} [<i>{cites} citation(s)</i>] <a href="{pdf_url}" target="_blank">(pdf)</a> </td></tr>') %>%
str_replace_all("(, )+</p>", "</p>") %>%
char2html()
x <- c('<table><tbody>', x, '</tbody></table>')
return(x);
}) %>% rev
html_3 <- map2(names(html_2) %>% paste0("<h3>", ., "</h3>"), html_2, c) %>% unlist
html_4 <- c(
paste0('<p><i>Last updated ',
format(Sys.Date(), format="%d/%m/%Y"),
' &ndash; Pulled automatically from my <a href="https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en">Google Scholar profile</a>. See <a href="https://thackl.github.io/automatically-update-publications-with-R-scholar" target="_blank">this post</a> for how it works. See the code adapted to my usage <a href = "https://github.com/MaxLarter/maxlarter.github.io/blob/master/.R.Rmd" target="_blank">here</a>.</i></p>'), html_3)
dat <- noquote(readLines("index.html"))  # read in index.html to get header and footer setup
dat2 <- noquote(str_replace_all(dat, "[\\r\\n\\t]+", " ")) # remove the added characters cause it gets messy otherwise
dat3 <- noquote(str_replace_all(dat2, "[\"]", '"')) # remove "escaped" quotes
text_to_insert <- html_4 # this is the bit of text we want to add from before
insert_after <- which(dat %in% '\t<div id="main" role="main" class="container">') # line in the index.html where the header ends: insert pubs table after this
insert_before <- grep('<div class="wrapper-footer">', dat3) # this is the line where the index.html file footer starts: insert before this
# function to do the insertion
insert_line_at <- function(dat, to_insert, insert_after, insert_before){
pre <- dat[1:insert_after]
post <- dat[(insert_before):length(dat)]
return(c(pre, to_insert, post))
}
# run the insertion
dat_inserted <- insert_line_at(dat3, text_to_insert, insert_after=insert_after, insert_before = insert_before)
dat_inserted2 = gsub("\u00A0", " ", dat_inserted) # this removes the unbreakable space that appeared for some unkown reason...
writeLines(dat_inserted2, "publications/publications.html") # write to Publication list pages
######
# remember to commit & push to github and it's live!
dat_inserted2
pdf_urls_file
html_1
dat3
dat_inserted
dat_inserted2
html_1
html_1$title
# my google scholar user id from my profile url
# https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en
MaxLarter <- "-WJKy_EAAAAJ&hl"
# pull from google
html_1 <- get_publications(MaxLarter)
# pull pdf_urls matching GS titles
# first write this to file
write.csv(html_1 %>%
as_tibble %>% arrange(desc(year)), file = "publications/publist.csv")
# then load the pdf or URLs we want in the pub list # if you want to update new pub you NEED to add pdf link here or it will not show up
pdf_urls_file<-read.csv("publications/pdf_urls.csv", sep = ",", header = T)
html_1_2<-merge(html_1, pdf_urls_file, by = "title")
# convert to htlm table - the ugly way ;)
html_1_2 <- html_1_2 %>% mutate(author= ifelse(!str_detect(author, 'Larter'),
str_replace_all(author, ", \\.\\.\\.", ", M Larter, \\.\\.\\."),author)
) # this will add my name before the "et al." in long author lists when I'm towards the end
html_2 <- html_1_2 %>%
as_tibble %>% arrange(desc(year)) %>%
mutate(
#    author=str_replace_all(author, " (\\S) ", "\\1 "),
author=str_replace_all(author, "([A-Z]) ([A-Z]) ", "\\1\\2 "),
author=str_replace_all(author, ", \\.\\.\\.", " et al."),
author=str_replace_all(author, "M Larter", "<b>M Larter</b>"), # make my name fat
author=str_replace_all(author, "MD Larter", "<b>MD Larter</b>") # make my name fat
) %>% split(.$year) %>%
map(function(x){
x <- x %>%
glue_data('<tr><td width="100%">{author} ({year}) <a href="https://scholar.google.com/scholar?oi=bibs&cluster={cid}&btnI=1&hl=en" target="_blank">{title}</a>, {journal}, {number} [<i>{cites} citation(s)</i>] <a href="{pdf_url}" target="_blank">(pdf)</a> </td></tr>') %>%
str_replace_all("(, )+</p>", "</p>") %>%
char2html()
x <- c('<table><tbody>', x, '</tbody></table>')
return(x);
}) %>% rev
html_3 <- map2(names(html_2) %>% paste0("<h3>", ., "</h3>"), html_2, c) %>% unlist
html_4 <- c(
paste0('<p><i>Last updated ',
format(Sys.Date(), format="%d/%m/%Y"),
' &ndash; Pulled automatically from my <a href="https://scholar.google.com/citations?user=-WJKy_EAAAAJ&hl=en">Google Scholar profile</a>. See <a href="https://thackl.github.io/automatically-update-publications-with-R-scholar" target="_blank">this post</a> for how it works. See the code adapted to my usage <a href = "https://github.com/MaxLarter/maxlarter.github.io/blob/master/.R.Rmd" target="_blank">here</a>.</i></p>'), html_3)
dat <- noquote(readLines("index.html"))  # read in index.html to get header and footer setup
dat2 <- noquote(str_replace_all(dat, "[\\r\\n\\t]+", " ")) # remove the added characters cause it gets messy otherwise
dat3 <- noquote(str_replace_all(dat2, "[\"]", '"')) # remove "escaped" quotes
text_to_insert <- html_4 # this is the bit of text we want to add from before
insert_after <- which(dat %in% '\t<div id="main" role="main" class="container">') # line in the index.html where the header ends: insert pubs table after this
insert_before <- grep('<div class="wrapper-footer">', dat3) # this is the line where the index.html file footer starts: insert before this
# function to do the insertion
insert_line_at <- function(dat, to_insert, insert_after, insert_before){
pre <- dat[1:insert_after]
post <- dat[(insert_before):length(dat)]
return(c(pre, to_insert, post))
}
# run the insertion
dat_inserted <- insert_line_at(dat3, text_to_insert, insert_after=insert_after, insert_before = insert_before)
dat_inserted2 = gsub("\u00A0", " ", dat_inserted) # this removes the unbreakable space that appeared for some unkown reason...
writeLines(dat_inserted2, "publications/publications.html") # write to Publication list pages
######
# remember to commit & push to github and it's live!
co_auth<-get_coauthors(MaxLarter, n_coauthors = 5)
plot_coauthors(co_auth)
